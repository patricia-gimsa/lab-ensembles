{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9975797-fbde-4f96-8aac-85a4f950c421",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "\n",
    "The heart disease dataset is a classic dataset that contains various health metrics (age, sex, chest pain type, blood pressure, cholesterol, etc.) related to diagnosing heart disease (binary classification: presence or absence of heart disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cf591d-8a5b-499e-8715-1ad140867934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset (change the path if needed)\n",
    "df = pd.read_csv('../data/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb5ea1c-a4e5-4419-bae8-661fe2d82711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # Display the first 5 rows of the dataset to understand its structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ebc45-d873-4c37-b1e4-ce2b0ebc08f2",
   "metadata": {},
   "source": [
    "We are going to try to predict the presence of hart disease suing this features, starting with a classical baseline method and trying to improve on that result with a series of ensembled approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ad7e40-87f3-4b93-bef9-a9ddb5881ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "# We drop the column \"target\" from the dataset to get the features (X)\n",
    "# The \"target\" column is what we want to predict (1 = heart disease, 0 = no heart disease)\n",
    "X = df.drop(columns=\"target\")\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "# 25% of the data will be used for testing, 75% for training\n",
    "# random_state=0 ensures we get the same split every time\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scale the features (important for models like SVM, logistic regression, etc.)\n",
    "# Scaling puts all feature values on the same scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the same scaler (already fitted) to transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0153586-1242-43a0-bb61-78b1234434a6",
   "metadata": {},
   "source": [
    "# Baseline model : decision Tree\n",
    "\n",
    "We'll train a decision tree as our baseline model and evaluate it using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39376f1-b4ca-44c0-8364-d11b9a7605f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "#Create and Train a Decision Tree Classifier and print the train and test accuracy\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Train Decision Tree\n",
    "# Create the model\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "# Make predictions on both train and test sets\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f37ff0-c71e-40eb-803d-3069a2b41368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model has Train Accuracy = 100%, but the Test Accuracy is much lower. This means the model is overfitting: it memorizes the training data perfectly, but it does not generalize well to new data. Decision Trees can easily become too complex and too specific to the training data, which makes them unstable and high variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9502aa2-06b0-4cf2-bc6b-cb91390ff1a8",
   "metadata": {},
   "source": [
    "We can see that this model is overfitting. This is expected, decision trees, especially deep ones  are notorious agressive at exploiting the data available. But that also makes them highly variant: a small change on the tree/data makes for potentially large changes in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c60160a-b179-4896-a026-4beab803bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "# Run the same code again a couple of times. \n",
    "# You can see that the Train Accuracy is always 100% (overfitting) and the Test Accuracy is all over the place. \n",
    "# This is undesirable: our method is not generalizing and has high variance\n",
    "\n",
    "clf = DecisionTreeClassifier() # We remove the random_state so the tree will be slightly different on each run\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d29f337e-4315-4980-bccc-3c7800b7c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Decision Tree model always gets 100% accuracy on the training data, but never goes above 81% on the test data. This shows that the model is overfitting: it learns the training examples too well, but it doesn't generalize well to new data. We need a better method that reduces this overfitting problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb71cb-fc65-4c49-a1d6-1abd9a1085c1",
   "metadata": {},
   "source": [
    "# Bagging: reducing variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f3606-bb0b-4567-8583-11c38ab02579",
   "metadata": {},
   "source": [
    "Bagging improves models because it reduces variance by averaging the predictions of multiple models trained on different subsets of the training data. This averaging effect reduces the sensitivity of the overall model to any one dataset or model, making the final prediction more stable and less prone to overfitting.\n",
    "\n",
    "- High-variance models, like decision trees, tend to overfit the training data. This means that small changes in the training data can lead to large changes in the model’s predictions. For example, a decision tree trained on one subset of data might look completely different from a decision tree trained on another subset. This leads to high variance, where the model’s performance fluctuates a lot depending on the specific data it was trained on.\n",
    "- Once all the individual models are trained, Bagging combines their predictions by averaging them (for regression) or using a majority vote (for classification). The key idea here is that the errors in each individual model are somewhat independent because they are trained on different bootstrap samples. Some models will make errors in one direction, while others might make errors in another. When you average these predictions, the errors cancel out, reducing the overall variability (variance) of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fc76766-a90c-47ed-bd02-66827a1dc115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8193832599118943\n",
      "Test Accuracy: 0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "# Create and Train a BaggingClassifier. \n",
    "# Use as base estimator a weak decision tree (max_depth=1) and 100 estimators to really over a lot of different data samples\n",
    "# Print the train and test accuracy\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Train BaggingClassifier\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),  # weak learner (shallow tree)\n",
    "    n_estimators=100,  # number of trees\n",
    "    random_state=0)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_train_pred = bagging_model.predict(X_train)\n",
    "y_test_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feca08ed-e9e4-44a0-9882-06db57c30561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Bagging, the Train Accuracy is lower than before, but the Test Accuracy is better and more stable. This means the model is not overfitting anymore. Bagging helps by training many simple trees and averaging their predictions. This reduces the model's sensitivity to the training data and makes it more reliable on new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efabafd-e27b-4a70-85e2-159170853f0b",
   "metadata": {},
   "source": [
    "You can probably see a modest improvement in score, but most importantly, the overfitting is mostly gone. This is because averaging over multiple datasets stabilizes the high variance of the base model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f892484-618a-46fe-8e56-0a18fa652ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8325991189427313\n",
      "Test Accuracy: 0.8552631578947368\n"
     ]
    }
   ],
   "source": [
    "# Run the same code again a couple of times. \n",
    "# You can see that consistently the Train Accuracy is close to the Test Accuracy. \n",
    "\n",
    "# Create and Train a BaggingClassifier. \n",
    "# Use as base estimator a weak decision tree (max_depth=1) and 100 estimators to really over a lot of different data samples\n",
    "# Print the train and test accuracy\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Train BaggingClassifier no random_state\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),  # weak learner (shallow tree)\n",
    "    n_estimators=100) # number of trees\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_train_pred = bagging_model.predict(X_train)\n",
    "y_test_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e143846-a1d9-4e5a-b8ab-ced83437ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When we remove the random_state, the results change each time we run the model. However, we can still see that the Train Accuracy and Test Accuracy stay very close to each other. This means the model is stable and not overfitting. Bagging helps reduce variance by training on different subsets of data and combining their results, so the performance does not depend too much on a specific training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99849f-20fe-4eac-be80-b43dd56ba374",
   "metadata": {},
   "source": [
    "# Boosting: reducing bias\n",
    "\n",
    "Now we’ll apply AdaBoost with decision trees as weak learners. This will sequentially improve the model by focusing on difficult cases.\n",
    "\n",
    "Boosting reduces bias by sequentially training a series of weak learners (often simple models like decision trees) where each subsequent model focuses on the mistakes made by the previous models. The key idea behind boosting is to incrementally improve the model by correcting errors, which helps to reduce bias, especially when the initial model is too simple and underfits the data.\n",
    "\n",
    "- Boosting typically uses weak learners, which are models that perform only slightly better than random guessing. For example, in classification, a weak learner might be a shallow decision tree (a \"stump\") with just a few levels. Weak learners usually have high bias, meaning they are too simplistic and don't capture the underlying patterns in the data well. As a result, they underfit the data.\n",
    "\n",
    "- In each iteration, boosting trains a new model that tries to correct the errors made by the earlier models. If an instance was misclassified by the first weak learner, it will receive a higher weight, so the next model pays more attention to it. As the sequence of models progresses, the ensemble collectively focuses more on the difficult-to-predict instances. Over time, the combined models become better at fitting the data, as they successively reduce the bias (systematic error) by adjusting for earlier mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bba1773-b0b0-44ba-a838-58b8c466ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9118942731277533\n",
      "Test Accuracy: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "# Create and Train a AdaBoostClassifier. \n",
    "# Use as base estimator a weak decision tree (max_depth=1) and 100 estimators to really target the specific behaviors of this phenomenon\n",
    "# Print the train and test accuracy\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Train AdaBoost\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),  # very simple tree (weak learner)\n",
    "    n_estimators=100  # number of boosting rounds (trees)\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_train_pred = adaboost_model.predict(X_train)\n",
    "y_test_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad0133c4-c21d-4650-a1ac-67990d70a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost increases the training accuracy significantly, but the test accuracy is only slightly better than Bagging. \n",
    "# This small gap between train and test suggests that overfitting is coming back, although not as strongly as with a single decision tree.\n",
    "# AdaBoost focuses on hard examples, which helps the model fit the training data well, but can hurt generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427e8a3-478b-4260-b2ac-74aa80983e50",
   "metadata": {},
   "source": [
    "You can probably see a good improvement in score, but overfitting rearing it's ugly head a gain (not as much as in the base model). This is because the iterative correction of adaboost really allows the model to focus on the specifics of this problem, at a cost of overexploiting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b5e21fe-0a8f-45f6-a2d3-74261941f9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9118942731277533\n",
      "Test Accuracy: 0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "# Run the same code again a couple of times. S\n",
    "# You can see that the test Accuracy will mostly be pretty good, even if some times it get's lower or higher scores (high variance, low bias)\n",
    "# You can see also that consistently the Train Accuracy is higher than the Test Accuracy,indicating some (not extreme) overfitting \n",
    "\n",
    "np.random.seed(None)  # force different randomness on each run\n",
    "\n",
    "# Train AdaBoost\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),  # very simple tree (weak learner)\n",
    "    n_estimators=100  # number of boosting rounds (trees)\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_train_pred = adaboost_model.predict(X_train)\n",
    "y_test_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "102b508c-86e8-4227-952e-21095618d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost is a deterministic algorithm, so running the same code multiple times gives the same result.\n",
    "# Unlike Bagging, there is no randomness in how AdaBoost samples data. It builds each tree step-by-step \n",
    "# based on the previous model's errors, so the outcome is stable as long as the input data stays the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
